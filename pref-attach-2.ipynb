{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700c0311",
   "metadata": {},
   "source": [
    "# Second attempt: Preferential attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd94a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "G = pickle.load(open('data/ai_ecosystem_graph_finetune.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95cf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preferential-attachment estimation on a time-stamped lineage tree (NetworkX)\n",
    "# Author: You (+ a professor friend)\n",
    "# Requirements: networkx, pandas, numpy\n",
    "\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "\n",
    "def _to_timestamp(x):\n",
    "    # Parse ISO8601 strings like '2024-08-15T18:50:44.000Z'; return pd.Timestamp (UTC)\n",
    "    try:\n",
    "        return pd.to_datetime(x, utc=True)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def _is_directed_parent_to_child(G: nx.DiGraph) -> bool:\n",
    "    # We assume edges go parent -> child (parent has out-edge to child).\n",
    "    # If your graph uses the reverse, flip when building events.\n",
    "    return isinstance(G, nx.DiGraph)\n",
    "\n",
    "def _children_times_by_parent(G, created):\n",
    "    \"\"\"Return dict: parent -> sorted list of child creation times (pd.Timestamp).\"\"\"\n",
    "    d = defaultdict(list)\n",
    "    for u, v in G.edges():\n",
    "        ct = created.get(v, pd.NaT)\n",
    "        if pd.notna(ct):\n",
    "            d[u].append(ct)\n",
    "    for u in d:\n",
    "        d[u].sort()\n",
    "    return d\n",
    "\n",
    "def _initial_degree_before(parent, times_list, cutoff):\n",
    "    \"\"\"#children with creation time < cutoff.\"\"\"\n",
    "    # times_list is sorted; binary search\n",
    "    import bisect\n",
    "    return bisect.bisect_left(times_list, cutoff)\n",
    "\n",
    "def _bin_edges_log2(max_k_seen: int, include_zero=True):\n",
    "    \"\"\"\n",
    "    Create compact log2 bins:\n",
    "      [0], [1], [2-3], [4-7], [8-15], ...\n",
    "    \"\"\"\n",
    "    edges = [0] if include_zero else []\n",
    "    # upper edges are inclusive (we’ll store as (lo, hi))\n",
    "    hi = 1\n",
    "    edges.append(1)\n",
    "    while hi < max(1, max_k_seen):\n",
    "        lo = hi + 1\n",
    "        hi = 2*hi + 1  # 1->3, 3->7, 7->15, ...\n",
    "        edges.append(hi)\n",
    "    # Convert cumulative upper-edges into (lo, hi) bins later\n",
    "    return edges\n",
    "\n",
    "def _degree_to_bin(k: int, bins: list[tuple[int,int]]):\n",
    "    # bins = list of (lo, hi) inclusive; return index\n",
    "    # k is nonnegative int\n",
    "    # Assumes bins cover all k you’ll actually see; we’ll extend if needed.\n",
    "    lo = 0; hi = len(bins)-1\n",
    "    while lo <= hi:\n",
    "        mid = (lo+hi)//2\n",
    "        a,b = bins[mid]\n",
    "        if k < a:\n",
    "            hi = mid-1\n",
    "        elif k > b:\n",
    "            lo = mid+1\n",
    "        else:\n",
    "            return mid\n",
    "    return None\n",
    "\n",
    "def _build_bins(max_k_seen: int):\n",
    "    # Build [(0,0),(1,1),(2,3),(4,7),...]\n",
    "    uppers = _bin_edges_log2(max_k_seen, include_zero=True)\n",
    "    bins = []\n",
    "    prev_hi = -1\n",
    "    for hi in uppers:\n",
    "        lo = prev_hi+1\n",
    "        bins.append((lo, hi))\n",
    "        prev_hi = hi\n",
    "    return bins\n",
    "\n",
    "# ----------------------------\n",
    "# Core estimator\n",
    "# ----------------------------\n",
    "\n",
    "def estimate_preferential_attachment_kernel(\n",
    "    G: nx.DiGraph,\n",
    "    start=\"2023-01-01\",          # analysis window start (inclusive)\n",
    "    end=\"2023-12-31 23:59:59\",   # analysis window end (inclusive)\n",
    "    k0: float = 1.0,             # degree offset to handle k=0\n",
    "    use_bins: bool = True,       # log2 degree binning for scalability\n",
    "    bins: list[tuple[int,int]] | None = None,  # custom bins if provided\n",
    "    drop_nodes_with_missing_time: bool = True,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      dict with keys:\n",
    "        - summary: DataFrame over bins (or degrees) with exposures N, choices M, Ahat\n",
    "        - alpha: slope from log-fit of Ahat vs log(k+k0)\n",
    "        - fit: dict with slope, intercept, r2\n",
    "        - bins: bin ranges [(lo,hi),...]\n",
    "    \"\"\"\n",
    "\n",
    "    assert _is_directed_parent_to_child(G), \"Expected a DiGraph with edges parent->child.\"\n",
    "\n",
    "    # 0) Parse creation times\n",
    "    created = {}\n",
    "    for n, d in G.nodes(data=True):\n",
    "        ts = _to_timestamp(d.get(\"createdAt\", None))\n",
    "        if pd.isna(ts) and drop_nodes_with_missing_time:\n",
    "            continue\n",
    "        created[n] = ts\n",
    "\n",
    "    # Filter the graph to nodes with known times (optional)\n",
    "    if drop_nodes_with_missing_time:\n",
    "        keep = set(created.keys())\n",
    "        G = G.subgraph(keep).copy()\n",
    "\n",
    "    # 1) Identify roots (no parent) and build events (attachments) within [start, end]\n",
    "    start_ts = pd.to_datetime(start, utc=True)\n",
    "    end_ts   = pd.to_datetime(end,   utc=True)\n",
    "\n",
    "    # parent list for each child (tree → 0 or 1)\n",
    "    parent_of = {}\n",
    "    for u, v in G.edges():\n",
    "        if v in parent_of:  # safety for malformed trees\n",
    "            # keep the earliest parent or the only one; your data should have exactly one\n",
    "            pass\n",
    "        parent_of[v] = u\n",
    "\n",
    "    # Attachments (child nodes with a parent) in window\n",
    "    events = []\n",
    "    for child, p in parent_of.items():\n",
    "        tc = created.get(child, pd.NaT)\n",
    "        if pd.isna(tc):\n",
    "            continue\n",
    "        if start_ts <= tc <= end_ts:\n",
    "            # Ensure parent existed before child\n",
    "            tp = created.get(p, pd.NaT)\n",
    "            if pd.isna(tp) or tp >= tc:\n",
    "                # parent missing or future-created (shouldn't happen in well-formed lineage)\n",
    "                continue\n",
    "            events.append((tc, p, child))\n",
    "\n",
    "    events.sort(key=lambda x: x[0])\n",
    "    E = len(events)\n",
    "    if verbose:\n",
    "        print(f\"# attachment events in window: {E}\")\n",
    "\n",
    "    # Roots created inside window (these join the eligible set with degree 0)\n",
    "    roots_in_window = []\n",
    "    for n in G.nodes():\n",
    "        if G.in_degree(n) == 0:\n",
    "            tn = created.get(n, pd.NaT)\n",
    "            if pd.notna(tn) and (start_ts <= tn <= end_ts):\n",
    "                roots_in_window.append((tn, n))\n",
    "    roots_in_window.sort(key=lambda x: x[0])\n",
    "\n",
    "    # 2) Build per-parent child-time lists and initial degrees before start\n",
    "    child_times = _children_times_by_parent(G, created)\n",
    "\n",
    "    # Initial eligible set: nodes created before start_ts\n",
    "    eligible = set(n for n, t in created.items() if pd.notna(t) and t < start_ts)\n",
    "\n",
    "    # Current dynamic degree dict (children strictly before \"now\")\n",
    "    curr_deg = {}\n",
    "    max_k_seen = 0\n",
    "    for n in eligible:\n",
    "        times = child_times.get(n, [])\n",
    "        k0_init = _initial_degree_before(n, times, start_ts)\n",
    "        curr_deg[n] = k0_init\n",
    "        if k0_init > max_k_seen:\n",
    "            max_k_seen = k0_init\n",
    "\n",
    "    # 3) Binning setup\n",
    "    if use_bins and bins is None:\n",
    "        bins = _build_bins(max_k_seen=max_k_seen)\n",
    "    elif not use_bins:\n",
    "        bins = None  # we will index by exact k\n",
    "\n",
    "    # Histogram over eligible set at current time (before first event)\n",
    "    if use_bins:\n",
    "        # counts per bin index\n",
    "        H = np.zeros(len(bins), dtype=np.int64)\n",
    "        for n in eligible:\n",
    "            k = curr_deg[n]\n",
    "            # extend bins if larger degree appears later\n",
    "            while bins[-1][1] < k:\n",
    "                # grow bins on the fly\n",
    "                lo, hi = bins[-1]\n",
    "                bins.append((hi+1, 2*hi+1))\n",
    "                H = np.pad(H, (0,1))\n",
    "            bi = _degree_to_bin(k, bins)\n",
    "            H[bi] += 1\n",
    "    else:\n",
    "        # sparse dict: degree k -> count\n",
    "        H = defaultdict(int)\n",
    "        for n in eligible:\n",
    "            H[curr_deg[n]] += 1\n",
    "\n",
    "    # Exposures N(k) and choices M(k)\n",
    "    if use_bins:\n",
    "        N = np.zeros_like(H, dtype=np.int64)  # exposures\n",
    "        M = np.zeros_like(H, dtype=np.int64)  # chosen counts\n",
    "    else:\n",
    "        N = defaultdict(int)\n",
    "        M = defaultdict(int)\n",
    "\n",
    "    # 4) Walk events in time; maintain:\n",
    "    #    - inject new roots before each attachment event\n",
    "    #    - expose current H to this event\n",
    "    #    - record chosen parent’s current degree\n",
    "    #    - update H for parent move k->k+1 and new child entering with k=0\n",
    "\n",
    "    root_ptr = 0\n",
    "    for e_idx, (t, p, child) in enumerate(events, start=1):\n",
    "\n",
    "        # Add any roots born before this event (they become eligible with k=0)\n",
    "        while root_ptr < len(roots_in_window) and roots_in_window[root_ptr][0] < t:\n",
    "            _, r = roots_in_window[root_ptr]\n",
    "            root_ptr += 1\n",
    "            if r in eligible:\n",
    "                continue  # already eligible (shouldn't happen)\n",
    "            eligible.add(r)\n",
    "            curr_deg[r] = 0\n",
    "            if use_bins:\n",
    "                bi0 = _degree_to_bin(0, bins)\n",
    "                H[bi0] += 1\n",
    "            else:\n",
    "                H[0] += 1\n",
    "\n",
    "        # Exposures at this event = current H just before choosing a parent\n",
    "        if use_bins:\n",
    "            N += H\n",
    "        else:\n",
    "            for k, c in H.items():\n",
    "                N[k] += c\n",
    "\n",
    "        # Degree of chosen parent (before increment)\n",
    "        kp = curr_deg.get(p, None)\n",
    "        if kp is None:\n",
    "            # Parent might have been created after start but before now (eligible); ensure entry\n",
    "            if created.get(p, pd.NaT) < t:\n",
    "                curr_deg[p] = 0\n",
    "                kp = 0\n",
    "                eligible.add(p)\n",
    "                if use_bins:\n",
    "                    bi0 = _degree_to_bin(0, bins)\n",
    "                    H[bi0] += 1\n",
    "                else:\n",
    "                    H[0] += 1\n",
    "            else:\n",
    "                # parent not eligible yet (should not happen in a well-formed lineage window)\n",
    "                continue\n",
    "\n",
    "        # Record the choice at degree kp\n",
    "        if use_bins:\n",
    "            # Ensure bins cover kp and kp+1\n",
    "            while bins[-1][1] < kp+1:\n",
    "                lo, hi = bins[-1]\n",
    "                bins.append((hi+1, 2*hi+1))\n",
    "                H = np.pad(H, (0,1))\n",
    "                N = np.pad(N, (0,1))\n",
    "                M = np.pad(M, (0,1))\n",
    "            bi_k  = _degree_to_bin(kp,   bins)\n",
    "            bi_k1 = _degree_to_bin(kp+1, bins)\n",
    "            M[bi_k] += 1\n",
    "            # Update histogram for parent move kp -> kp+1\n",
    "            H[bi_k]  -= 1\n",
    "            H[bi_k1] += 1\n",
    "        else:\n",
    "            M[kp] += 1\n",
    "            H[kp]  -= 1\n",
    "            H[kp+1]+= 1\n",
    "\n",
    "        curr_deg[p] = kp + 1\n",
    "        max_k_seen = max(max_k_seen, kp+1)\n",
    "\n",
    "        # New child joins eligible set with degree 0 for future events\n",
    "        # (child's own event time is now; it should NOT be eligible for this event)\n",
    "        curr_deg[child] = 0\n",
    "        eligible.add(child)\n",
    "        if use_bins:\n",
    "            bi0 = _degree_to_bin(0, bins)\n",
    "            H[bi0] += 1\n",
    "        else:\n",
    "            H[0] += 1\n",
    "\n",
    "    # 5) Build summary table and fit alpha from Ahat(k) ∝ (k+k0)^alpha\n",
    "    eps = 1e-9  # small smoothing for empty cells\n",
    "    rows = []\n",
    "    if use_bins:\n",
    "        for j, (lo, hi) in enumerate(bins):\n",
    "            Nj, Mj = int(N[j]), int(M[j])\n",
    "            if Nj == 0 and Mj == 0:\n",
    "                continue\n",
    "            # representative degree for the bin: geometric mid (avoid zero with +k0)\n",
    "            if lo == hi:\n",
    "                k_rep = lo\n",
    "            else:\n",
    "                # geometric mean if both >0; else fallback to (lo+hi)/2\n",
    "                if lo > 0:\n",
    "                    k_rep = math.sqrt(lo*hi)\n",
    "                else:\n",
    "                    k_rep = (lo + hi)/2.0\n",
    "            Ahat = (Mj + eps) / (Nj + eps)\n",
    "            rows.append({\"bin_lo\": lo, \"bin_hi\": hi, \"k_rep\": k_rep, \"N_exposure\": Nj, \"M_choices\": Mj, \"Ahat\": Ahat})\n",
    "    else:\n",
    "        for k, Nj in N.items():\n",
    "            Mj = M.get(k, 0)\n",
    "            if Nj == 0 and Mj == 0:\n",
    "                continue\n",
    "            Ahat = (Mj + eps) / (Nj + eps)\n",
    "            rows.append({\"bin_lo\": k, \"bin_hi\": k, \"k_rep\": float(k), \"N_exposure\": int(Nj), \"M_choices\": int(Mj), \"Ahat\": Ahat})\n",
    "\n",
    "    summary = pd.DataFrame(rows).sort_values([\"bin_lo\", \"bin_hi\"]).reset_index(drop=True)\n",
    "\n",
    "    # Fit alpha via weighted least squares on log-log:\n",
    "    # log Ahat = c + alpha * log(k_rep + k0)\n",
    "    if len(summary) == 0:\n",
    "        raise ValueError(\"No events/exposures in the specified window after filtering.\")\n",
    "\n",
    "    x = np.log(summary[\"k_rep\"].values + k0)\n",
    "    y = np.log(summary[\"Ahat\"].values)\n",
    "    # weights: use exposures or choices; choices stabilizes for sparse high-k\n",
    "    w = np.maximum(1.0, summary[\"M_choices\"].values.astype(float))\n",
    "    # numpy polyfit with weights fits y = a*x + b\n",
    "    alpha, intercept = np.polyfit(x, y, deg=1, w=w)\n",
    "    # Goodness-of-fit R^2 (weighted)\n",
    "    yhat = alpha * x + intercept\n",
    "    wmean = np.average(y, weights=w)\n",
    "    ss_tot = np.sum(w * (y - wmean)**2)\n",
    "    ss_res = np.sum(w * (y - yhat)**2)\n",
    "    r2 = 1.0 - (ss_res / ss_tot if ss_tot > 0 else np.nan)\n",
    "\n",
    "    fit = {\"alpha\": float(alpha), \"intercept\": float(intercept), \"r2_loglog\": float(r2)}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Estimated alpha = {alpha:.3f} (R^2 on log–log = {r2:.3f}, k0={k0})\")\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"alpha\": float(alpha),\n",
    "        \"fit\": fit,\n",
    "        \"bins\": bins,\n",
    "        \"k0\": k0,\n",
    "        \"window\": (pd.to_datetime(start_ts), pd.to_datetime(end_ts)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2813c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6144723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, pandas as pd, networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "def _ts(x):\n",
    "    try: return pd.to_datetime(x, utc=True)\n",
    "    except: return pd.NaT\n",
    "\n",
    "def _children_times_by_parent(G, created):\n",
    "    d = defaultdict(list)\n",
    "    for u,v in G.edges():\n",
    "        tv = created.get(v, pd.NaT)\n",
    "        if pd.notna(tv): d[u].append(tv)\n",
    "    for u in d: d[u].sort()\n",
    "    return d\n",
    "\n",
    "def _initial_degree_before(times_list, cutoff):\n",
    "    import bisect\n",
    "    return bisect.bisect_left(times_list, cutoff)\n",
    "\n",
    "def _build_bins(max_k):\n",
    "    # [(0,0),(1,1),(2,3),(4,7),...]\n",
    "    if max_k < 1: return [(0,0),(1,1)]\n",
    "    bins=[(0,0),(1,1)]; lo, hi = 2,3\n",
    "    while hi < max_k:\n",
    "        bins.append((lo,hi))\n",
    "        lo,hi = hi+1, 2*hi+1\n",
    "    if bins[-1][1] < max_k: bins.append((lo, max_k))\n",
    "    return bins\n",
    "\n",
    "def _bin_index(k, bins):\n",
    "    # bins are inclusive (lo,hi)\n",
    "    lo, hi = 0, len(bins)-1\n",
    "    while lo <= hi:\n",
    "        m=(lo+hi)//2; a,b=bins[m]\n",
    "        if k<a: hi=m-1\n",
    "        elif k>b: lo=m+1\n",
    "        else: return m\n",
    "    return None\n",
    "\n",
    "def estimate_pa_kernel_simple(\n",
    "    G: nx.DiGraph,\n",
    "    start=\"2023-01-01\",\n",
    "    end=\"2023-12-31 23:59:59\",\n",
    "    k0: float = 1.0,\n",
    "    use_bins: bool = True,\n",
    "    max_events: int | None = None,       # process only first N events (fast sanity check)\n",
    "    restrict_root: str | None = None,    # analyze a single family (root and descendants)\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"Estimate A(k) ∝ (k+k0)^alpha via exposure–event ratio, without handling new roots.\"\"\"\n",
    "    assert isinstance(G, nx.DiGraph), \"Expect parent->child DiGraph\"\n",
    "\n",
    "    # Parse times\n",
    "    created = {n:_ts(d.get(\"createdAt\")) for n,d in G.nodes(data=True)}\n",
    "    keep = {n for n,t in created.items() if pd.notna(t)}\n",
    "    G = G.subgraph(keep).copy()\n",
    "\n",
    "    # Optional: restrict to one family (root + descendants)\n",
    "    if restrict_root is not None:\n",
    "        if restrict_root not in G: raise ValueError(\"restrict_root not in graph\")\n",
    "        fam = nx.descendants(G, restrict_root) | {restrict_root}\n",
    "        G = G.subgraph(fam).copy()\n",
    "        created = {n:created[n] for n in G.nodes()}\n",
    "\n",
    "    # Parent map (tree: unique parent per child)\n",
    "    parent_of = {}\n",
    "    for u,v in G.edges():\n",
    "        if v in parent_of:  # keep earliest parent if duplicates exist\n",
    "            # prefer earliest-created parent\n",
    "            if created.get(u, pd.Timestamp.max) < created.get(parent_of[v], pd.Timestamp.max):\n",
    "                parent_of[v]=u\n",
    "        else:\n",
    "            parent_of[v]=u\n",
    "\n",
    "    start_ts = pd.to_datetime(start, utc=True)\n",
    "    end_ts   = pd.to_datetime(end,   utc=True)\n",
    "\n",
    "    # Events = (time, parent, child) where child is born in [start,end] and parent existed earlier\n",
    "    events=[]\n",
    "    for child,p in parent_of.items():\n",
    "        tc = created.get(child, pd.NaT)\n",
    "        tp = created.get(p, pd.NaT)\n",
    "        if pd.notna(tc) and pd.notna(tp) and start_ts <= tc <= end_ts and tp < tc:\n",
    "            events.append((tc,p,child))\n",
    "    events.sort(key=lambda x:x[0])\n",
    "    if max_events is not None:\n",
    "        events = events[:max_events]\n",
    "    if verbose: print(f\"# events processed: {len(events)}\")\n",
    "\n",
    "    # Eligible at window start: all nodes created before start\n",
    "    child_times = _children_times_by_parent(G, created)\n",
    "    eligible = {n for n,t in created.items() if t < start_ts}\n",
    "    curr_deg = {}\n",
    "    max_k_seen = 0\n",
    "    for n in eligible:\n",
    "        k = _initial_degree_before(child_times.get(n,[]), start_ts)\n",
    "        curr_deg[n] = k\n",
    "        if k > max_k_seen: max_k_seen = k\n",
    "\n",
    "    # Initialize degree histogram H\n",
    "    if use_bins:\n",
    "        bins = _build_bins(max_k_seen)\n",
    "        H = np.zeros(len(bins), dtype=np.int64)\n",
    "        def ensure_cover(k):\n",
    "            nonlocal bins, H\n",
    "            while k > bins[-1][1]:\n",
    "                lo=bins[-1][1]+1; hi=2*bins[-1][1]+1\n",
    "                bins.append((lo,hi)); H = np.pad(H,(0,1))\n",
    "        for n in eligible:\n",
    "            ensure_cover(curr_deg[n])\n",
    "            H[_bin_index(curr_deg[n],bins)] += 1\n",
    "        N = np.zeros_like(H); M = np.zeros_like(H)\n",
    "    else:\n",
    "        bins=None\n",
    "        H=defaultdict(int); N=defaultdict(int); M=defaultdict(int)\n",
    "        for n in eligible: H[curr_deg[n]] += 1\n",
    "\n",
    "    # Sweep events\n",
    "    for t,p,child in events:\n",
    "        # Exposure at this event\n",
    "        if use_bins:\n",
    "            N += H\n",
    "        else:\n",
    "            for k,c in H.items(): N[k] += c\n",
    "\n",
    "        # Ensure parent is eligible (it might have been born after start_ts)\n",
    "        if p not in curr_deg:\n",
    "            curr_deg[p] = _initial_degree_before(child_times.get(p,[]), t)\n",
    "            # add to histogram\n",
    "            if use_bins:\n",
    "                ensure_cover(curr_deg[p])\n",
    "                H[_bin_index(curr_deg[p],bins)] += 1\n",
    "            else:\n",
    "                H[curr_deg[p]] += 1\n",
    "            eligible.add(p)\n",
    "\n",
    "        kp = curr_deg[p]\n",
    "        # Record the choice and update parent degree\n",
    "        if use_bins:\n",
    "            ensure_cover(kp+1)\n",
    "            b0 = _bin_index(kp, bins); b1=_bin_index(kp+1, bins)\n",
    "            M[b0] += 1\n",
    "            H[b0] -= 1; H[b1] += 1\n",
    "        else:\n",
    "            M[kp] += 1\n",
    "            H[kp] -= 1; H[kp+1] += 1\n",
    "        curr_deg[p] = kp+1\n",
    "        if kp+1 > max_k_seen: max_k_seen = kp+1\n",
    "\n",
    "        # New child joins eligible at degree 0 for *future* events\n",
    "        curr_deg[child] = 0\n",
    "        if use_bins:\n",
    "            ensure_cover(0)\n",
    "            H[_bin_index(0,bins)] += 1\n",
    "        else:\n",
    "            H[0] += 1\n",
    "        eligible.add(child)\n",
    "\n",
    "    # Build summary and fit alpha on log–log\n",
    "    eps = 1e-9\n",
    "    rows=[]\n",
    "    if use_bins:\n",
    "        for (lo,hi), Nj, Mj in zip(bins, N, M):\n",
    "            if Nj==0 and Mj==0: continue\n",
    "            krep = lo if lo==hi else (math.sqrt(lo*hi) if lo>0 else (lo+hi)/2.0)\n",
    "            Ahat = (Mj+eps)/(Nj+eps)\n",
    "            rows.append({\"bin_lo\":lo,\"bin_hi\":hi,\"k_rep\":krep,\"N_exposure\":int(Nj),\"M_choices\":int(Mj),\"Ahat\":Ahat})\n",
    "    else:\n",
    "        for k,Nk in N.items():\n",
    "            Mk=M.get(k,0)\n",
    "            if Nk==0 and Mk==0: continue\n",
    "            rows.append({\"bin_lo\":k,\"bin_hi\":k,\"k_rep\":float(k),\"N_exposure\":int(Nk),\"M_choices\":int(Mk),\"Ahat\":(Mk+eps)/(Nk+eps)})\n",
    "    summary = pd.DataFrame(rows).sort_values([\"bin_lo\",\"bin_hi\"]).reset_index(drop=True)\n",
    "    if len(summary)==0: raise ValueError(\"No events/exposures in window.\")\n",
    "\n",
    "    x = np.log(summary[\"k_rep\"].values + k0)\n",
    "    y = np.log(summary[\"Ahat\"].values)\n",
    "    w = np.maximum(1.0, summary[\"M_choices\"].astype(float).values)\n",
    "    alpha, intercept = np.polyfit(x, y, deg=1, w=w)\n",
    "    yhat = alpha*x + intercept\n",
    "    wmean = np.average(y, weights=w)\n",
    "    r2 = 1 - (np.sum(w*(y-yhat)**2) / np.sum(w*(y-wmean)**2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"alpha={alpha:.3f}, R^2(log–log)={r2:.3f}, k0={k0}\")\n",
    "\n",
    "    return {\"summary\":summary, \"alpha\":float(alpha),\n",
    "            \"fit\":{\"alpha\":float(alpha),\"intercept\":float(intercept),\"r2_loglog\":float(r2)},\n",
    "            \"k0\":k0, \"bins\":bins}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c1a4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# events processed: 7\n",
      "alpha=-3.018, R^2(log–log)=0.126, k0=1.0\n",
      "   bin_lo  bin_hi  k_rep  N_exposure  M_choices          Ahat\n",
      "0       0       0    0.0          27          2  7.407407e-02\n",
      "1       1       1    1.0           2          2  1.000000e+00\n",
      "2       2       2    2.0           2          2  1.000000e+00\n",
      "3       3       3    3.0           1          1  1.000000e+00\n",
      "4       4       4    4.0           3          0  3.333333e-10\n",
      "{'alpha': -3.0179174682138012, 'intercept': -0.1115882359528393, 'r2_loglog': 0.1263538447707402}\n"
     ]
    }
   ],
   "source": [
    "def tiny_lineage():\n",
    "    # parent->child edges; createdAt encodes the order\n",
    "    G = nx.DiGraph()\n",
    "    mk = lambda t: {\"createdAt\": pd.Timestamp(t, tz=\"UTC\").isoformat()}\n",
    "    # Two parents born before 2023\n",
    "    G.add_node(\"A\", **mk(\"2022-12-01\"))\n",
    "    G.add_node(\"B\", **mk(\"2022-12-15\"))\n",
    "    # Children in 2023\n",
    "    G.add_node(\"C\", **mk(\"2023-01-10\")); G.add_edge(\"B\",\"C\")\n",
    "    G.add_node(\"D\", **mk(\"2023-01-10\")); G.add_edge(\"B\",\"D\")\n",
    "    G.add_node(\"E\", **mk(\"2023-01-20\")); G.add_edge(\"B\",\"E\")\n",
    "    G.add_node(\"F\", **mk(\"2023-02-01\")); G.add_edge(\"B\",\"F\")\n",
    "    G.add_node(\"G\", **mk(\"2023-02-05\")); G.add_edge(\"A\",\"G\")\n",
    "    G.add_node(\"H\", **mk(\"2023-02-05\")); G.add_edge(\"A\",\"H\")\n",
    "    G.add_node(\"I\", **mk(\"2023-02-05\")); G.add_edge(\"A\",\"I\")\n",
    "    return G\n",
    "\n",
    "Gt = tiny_lineage()\n",
    "res_tiny = estimate_pa_kernel_simple(Gt, start=\"2023-01-01\", end=\"2023-03-01\", k0=1.0, use_bins=False)\n",
    "print(res_tiny[\"summary\"])\n",
    "print(res_tiny[\"fit\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c79ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "def ymd(s):  # very fast date extractor\n",
    "    return s[:10] if (isinstance(s, str) and len(s)>=10) else None\n",
    "\n",
    "def collect_first_N_events(G, N, start_ymd=\"2023-01-01\", end_ymd=\"2023-12-31\"):\n",
    "    # Build parent_of once\n",
    "    parent_of = {}\n",
    "    for u,v in G.edges():\n",
    "        # keep earliest parent if duplicates\n",
    "        if v not in parent_of: parent_of[v]=u\n",
    "\n",
    "    # Gather children with creation in 2023\n",
    "    events = []  # (ymd_str, parent, child)\n",
    "    for child, p in parent_of.items():\n",
    "        ct = ymd(G.nodes[child].get(\"createdAt\"))\n",
    "        pt = ymd(G.nodes[p].get(\"createdAt\"))\n",
    "        if ct and pt and (start_ymd <= ct <= end_ymd) and (pt < ct):\n",
    "            events.append((ct, p, child))\n",
    "    events.sort(key=lambda x: x[0])\n",
    "    return events[:N]\n",
    "\n",
    "def build_parent_childtimes_for(events, G):\n",
    "    # Restrict bookkeeping to the parents that actually appear in the sampled events\n",
    "    parents = {p for _,p,_ in events}\n",
    "    child_times = {p: [] for p in parents}\n",
    "    # One pass over edges to fill only those parents\n",
    "    for u,v in G.edges():\n",
    "        if u in child_times:\n",
    "            tv = ymd(G.nodes[v].get(\"createdAt\"))\n",
    "            if tv: child_times[u].append(tv)\n",
    "    for p in child_times: child_times[p].sort()\n",
    "    return child_times\n",
    "\n",
    "def micro_pa_run(G, N=1000, k0=1.0, start_ymd=\"2023-01-01\", end_ymd=\"2023-12-31\"):\n",
    "    # 1) Sample events\n",
    "    events = collect_first_N_events(G, N, start_ymd, end_ymd)\n",
    "    # 2) Parent child-time lists (only for involved parents)\n",
    "    child_times = build_parent_childtimes_for(events, G)\n",
    "    # 3) Initial degree per involved parent at start\n",
    "    import bisect\n",
    "    curr_deg = {}\n",
    "    for p, lst in child_times.items():\n",
    "        curr_deg[p] = bisect.bisect_left(lst, start_ymd)  # children before start\n",
    "\n",
    "    # 4) Build a tiny histogram over **only** involved parents (sanity only)\n",
    "    # bins: 0,1,2-3,4-7,...\n",
    "    def build_bins(maxk):\n",
    "        if maxk < 1: return [(0,0),(1,1)]\n",
    "        b=[(0,0),(1,1)]\n",
    "        lo,hi=2,3\n",
    "        while hi < maxk: b.append((lo,hi)); lo,hi=hi+1, 2*hi+1\n",
    "        if b[-1][1] < maxk: b.append((lo,maxk))\n",
    "        return b\n",
    "    maxk = max(curr_deg.values()) if curr_deg else 0\n",
    "    bins = build_bins(maxk)\n",
    "    def bidx(k):\n",
    "        lo,hi=0,len(bins)-1\n",
    "        while lo<=hi:\n",
    "            m=(lo+hi)//2; a,b=bins[m]\n",
    "            if k<a: hi=m-1\n",
    "            elif k>b: lo=m+1\n",
    "            else: return m\n",
    "\n",
    "    H = np.zeros(len(bins), dtype=np.int64)  # histogram over involved parents only\n",
    "    for p,k in curr_deg.items(): H[bidx(k)] += 1\n",
    "    Nexp = np.zeros_like(H); Mch = np.zeros_like(H)\n",
    "\n",
    "    # 5) Sweep events (fast)\n",
    "    for ct, p, child in events:\n",
    "        # exposures among involved parents only (sanity)\n",
    "        Nexp += H\n",
    "        k = curr_deg[p]\n",
    "        bi = bidx(k)\n",
    "        Mch[bi] += 1\n",
    "        # parent degree increments\n",
    "        H[bi] -= 1\n",
    "        # ensure bins cover k+1\n",
    "        while bins[-1][1] < k+1:\n",
    "            lo=bins[-1][1]+1; hi=2*bins[-1][1]+1\n",
    "            bins.append((lo,hi))\n",
    "            H   = np.pad(H,(0,1)); Nexp=np.pad(Nexp,(0,1)); Mch=np.pad(Mch,(0,1))\n",
    "        H[bidx(k+1)] += 1\n",
    "        curr_deg[p] = k+1\n",
    "\n",
    "    # 6) Build Ahat and slope\n",
    "    rows=[]\n",
    "    for (lo,hi), Nj, Mj in zip(bins, Nexp, Mch):\n",
    "        if Nj==0 and Mj==0: continue\n",
    "        krep = lo if lo==hi else (np.sqrt(lo*hi) if lo>0 else (lo+hi)/2)\n",
    "        Ahat = (Mj+1e-9)/(Nj+1e-9)\n",
    "        rows.append((lo,hi,krep,int(Nj),int(Mj),Ahat))\n",
    "    df = pd.DataFrame(rows, columns=[\"bin_lo\",\"bin_hi\",\"k_rep\",\"N_exposure\",\"M_choices\",\"Ahat\"]).sort_values([\"bin_lo\",\"bin_hi\"])\n",
    "    x = np.log(df.k_rep.values + k0)\n",
    "    y = np.log(df.Ahat.values)\n",
    "    w = np.maximum(1.0, df.M_choices.values.astype(float))\n",
    "    alpha, c = np.polyfit(x, y, deg=1, w=w)\n",
    "    return df, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c86fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tiny = micro_pa_run(\n",
    "    G,\n",
    "    #start=\"2023-01-01\",\n",
    "    #end=\"2023-01-01 23:59:59\",\n",
    "    #k0=1.0,\n",
    "   # use_bins=True,\n",
    "   # max_events=100,    # try 100, 500, 2000\n",
    "   # verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "015b81ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   bin_lo  bin_hi       k_rep  N_exposure  M_choices      Ahat\n",
       " 0       0       0    0.000000       89725        197  0.002196\n",
       " 1       1       1    1.000000       95767         86  0.000898\n",
       " 2       2       3    2.449490       40162        102  0.002540\n",
       " 3       4       7    5.291503       21898         95  0.004338\n",
       " 4       8      15   10.954451       11088        117  0.010552\n",
       " 5      16      31   22.271057        6963        141  0.020250\n",
       " 6      32      55   41.952354        2040         89  0.043627\n",
       " 7      56     111   78.841613        1858        141  0.075888\n",
       " 8     112     223  158.037970         499         32  0.064128,\n",
       " np.float64(0.8312215599044414))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebde582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9b635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_small = estimate_pa_kernel_simple(\n",
    "    G,\n",
    "    start=\"2023-01-01\",\n",
    "    end=\"2023-01-01 23:59:59\",\n",
    "    k0=1.0,\n",
    "    use_bins=True,\n",
    "    max_events=100,    # try 100, 500, 2000\n",
    "    verbose=True\n",
    ")\n",
    "print(res_small[\"fit\"])\n",
    "res_small[\"summary\"].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b02f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ff3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95f845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7f487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba8a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc65620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783077f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ab04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbe536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6be4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_preferential_attachment_kernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-01-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-01-03 23:59:59\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#\"2023-12-31 23:59:59\",\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# set False if your 2023 event count is modest\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])          \u001b[38;5;66;03m# {'alpha': ..., 'intercept': ..., 'r2_loglog': ...}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m, in \u001b[0;36mestimate_preferential_attachment_kernel\u001b[0;34m(G, start, end, k0, use_bins, bins, drop_nodes_with_missing_time, verbose)\u001b[0m\n\u001b[1;32m    114\u001b[0m created \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, d \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 116\u001b[0m     ts \u001b[38;5;241m=\u001b[39m \u001b[43m_to_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreatedAt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(ts) \u001b[38;5;129;01mand\u001b[39;00m drop_nodes_with_missing_time:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36m_to_timestamp\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_timestamp\u001b[39m(x):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Parse ISO8601 strings like '2024-08-15T18:50:44.000Z'; return pd.Timestamp (UTC)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mNaT\n",
      "File \u001b[0;32m~/Python Projects/ai-preferential-attachment/.venv/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:1106\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mbool_):\n\u001b[1;32m   1108\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(result)  \u001b[38;5;66;03m# TODO: avoid this kludge.\u001b[39;00m\n",
      "File \u001b[0;32m~/Python Projects/ai-preferential-attachment/.venv/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:431\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    428\u001b[0m arg \u001b[38;5;241m=\u001b[39m ensure_object(arg)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_guess_datetime_format_for_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Python Projects/ai-preferential-attachment/.venv/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:133\u001b[0m, in \u001b[0;36m_guess_datetime_format_for_array\u001b[0;34m(arr, dayfirst)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (first_non_null \u001b[38;5;241m:=\u001b[39m tslib\u001b[38;5;241m.\u001b[39mfirst_non_null(arr)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first_non_nan_element \u001b[38;5;241m:=\u001b[39m arr[first_non_null]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;66;03m# GH#32264 np.str_ object\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m         guessed_format \u001b[38;5;241m=\u001b[39m \u001b[43mguess_datetime_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfirst_non_nan_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m guessed_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m guessed_format\n",
      "File \u001b[0;32mpandas/_libs/tslibs/parsing.pyx:1013\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.guess_datetime_format\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/tslibs/parsing.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing._fill_token\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/re.py:198\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern to all of the string, returning\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39mfullmatch(string)\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msearch(string)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = estimate_preferential_attachment_kernel(\n",
    "    G,\n",
    "    start=\"2023-01-01\",\n",
    "    end=\"2023-01-01 23:59:59\", #\"2023-12-31 23:59:59\",\n",
    "    k0=1.0,\n",
    "    use_bins=True,   # set False if your 2023 event count is modest\n",
    "    verbose=True\n",
    ")\n",
    "print(res[\"fit\"])          # {'alpha': ..., 'intercept': ..., 'r2_loglog': ...}\n",
    "print(res[\"summary\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e75d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0c8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fec1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04093e6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23ae598e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
